{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "829d1ff1-5b0d-4ca1-a58b-c7fa2606fd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Установка XGBoost\n",
    "# !pip install xgboost\n",
    "\n",
    "# # Установка CatBoost\n",
    "# !pip install catboost\n",
    "\n",
    "# # Установка TensorFlow\n",
    "# !pip install tensorflow\n",
    "\n",
    "# # Установка Scikeras \n",
    "# !pip install scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebaf5842-3613-4111-8e41-18d89ad3aa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers as L\n",
    "from scikeras.wrappers import KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcec2e73-b757-4c38-aaca-dd89d2ff9dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 'данные для моделей.csv' успешно загружен.\n"
     ]
    }
   ],
   "source": [
    "# Чтение DataFrame\n",
    "df_models = pd.read_csv(\"данные для моделей.csv\")\n",
    "print(\"DataFrame 'данные для моделей.csv' успешно загружен.\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39aab261-2ca6-49ed-a8a6-83479c617360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Подготовка данных для IC50 ---\n",
      "Размер обучающей выборки (X_train_ic50): (773, 68)\n",
      "Размер тестовой выборки (X_test_ic50): (194, 68)\n"
     ]
    }
   ],
   "source": [
    "#Подготовка данных для IC50\n",
    "# Целевая переменная для IC50\n",
    "target_ic50 = 'log_IC50, mM'\n",
    "\n",
    "# Признаки (X) и целевая переменная (y)\n",
    "# Исключаем другие целевые переменные из признаков\n",
    "X = df_models.drop(columns=['log_IC50, mM', 'log_CC50, mM', 'log_SI'])\n",
    "y = df_models[target_ic50]\n",
    "\n",
    "# Разделение на обучающую и тестовую выборки\n",
    "# random_state для воспроизводимости\n",
    "X_train_ic50, X_test_ic50, y_train_ic50, y_test_ic50 = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\n--- Подготовка данных для IC50 ---\")\n",
    "print(f\"Размер обучающей выборки (X_train_ic50): {X_train_ic50.shape}\")\n",
    "print(f\"Размер тестовой выборки (X_test_ic50): {X_test_ic50.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "484a4877-7237-4e42-a5d2-d6703564a919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Настройка Pipeline и GridSearchCV\n",
    "\n",
    "# Метрики для GridSearchCV. 'neg_' означает, что GridSearchCV стремится максимизировать метрику,\n",
    "scoring_metrics = {\n",
    "    'MSE': 'neg_mean_squared_error',\n",
    "    'R2': 'r2',\n",
    "    'MAE': 'neg_mean_absolute_error'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15c1fbab-f325-4fa5-bb06-2209922b29f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Линейная Регрессия для IC50 =====\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Лучшие параметры для Линейной Регрессии: {}\n",
      "Лучший R2 на кросс-валидации: 0.2813431323334865\n",
      "\n",
      "Метрики на тестовой выборке (Линейная Регрессия):\n",
      "MSE: 0.5935\n",
      "MAE: 0.6009\n",
      "R2: 0.2753\n"
     ]
    }
   ],
   "source": [
    "# Модель 1: Линейная Регрессия \n",
    "print(\"\\n===== Линейная Регрессия для IC50 =====\")\n",
    "pipeline_lr = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "param_grid_lr = {} # Для LinearRegression нет гиперпараметров \n",
    "\n",
    "grid_search_lr = GridSearchCV(\n",
    "    pipeline_lr,\n",
    "    param_grid_lr,\n",
    "    cv=5, # 5-кратная кросс-валидация\n",
    "    scoring=scoring_metrics,\n",
    "    refit='R2', # Оптимизировать по R2\n",
    "    n_jobs=-1, # Использовать все доступные ядра\n",
    "    verbose=1 # Выводить прогресс\n",
    ")\n",
    "\n",
    "grid_search_lr.fit(X_train_ic50, y_train_ic50)\n",
    "\n",
    "print(\"Лучшие параметры для Линейной Регрессии:\", grid_search_lr.best_params_)\n",
    "print(\"Лучший R2 на кросс-валидации:\", grid_search_lr.best_score_)\n",
    "\n",
    "# Оценка на тестовой выборке\n",
    "y_pred_lr = grid_search_lr.predict(X_test_ic50)\n",
    "mse_lr = mean_squared_error(y_test_ic50, y_pred_lr)\n",
    "r2_lr = r2_score(y_test_ic50, y_pred_lr)\n",
    "mae_lr = mean_absolute_error(y_test_ic50, y_pred_lr)\n",
    "\n",
    "print(\"\\nМетрики на тестовой выборке (Линейная Регрессия):\")\n",
    "print(f\"MSE: {mse_lr:.4f}\")\n",
    "print(f\"MAE: {mae_lr:.4f}\")\n",
    "print(f\"R2: {r2_lr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d2e499a-ba9e-40fa-b04c-08d1c9b35a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Random Forest Регрессор для IC50 =====\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Лучшие параметры для Random Forest: {'regressor__max_features': 0.6, 'regressor__min_samples_leaf': 5, 'regressor__n_estimators': 200}\n",
      "Лучший R2 на кросс-валидации: 0.4292621189253373\n",
      "\n",
      "Метрики на тестовой выборке (Random Forest):\n",
      "MSE: 0.4616\n",
      "MAE: 0.5325\n",
      "R2: 0.4364\n"
     ]
    }
   ],
   "source": [
    "# Модель 2: Random Forest Регрессор \n",
    "print(\"\\n===== Random Forest Регрессор для IC50 =====\")\n",
    "pipeline_rf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Гиперпараметры для настройки Random Forest\n",
    "param_grid_rf = {\n",
    "    'regressor__n_estimators': [50, 100, 200],\n",
    "    'regressor__max_features': [0.6, 0.8, 1.0],\n",
    "    'regressor__min_samples_leaf': [5, 10]\n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(\n",
    "    pipeline_rf,\n",
    "    param_grid_rf,\n",
    "    cv=5,\n",
    "    scoring=scoring_metrics,\n",
    "    refit='R2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_rf.fit(X_train_ic50, y_train_ic50)\n",
    "\n",
    "print(\"Лучшие параметры для Random Forest:\", grid_search_rf.best_params_)\n",
    "print(\"Лучший R2 на кросс-валидации:\", grid_search_rf.best_score_)\n",
    "\n",
    "# Оценка на тестовой выборке\n",
    "y_pred_rf = grid_search_rf.predict(X_test_ic50)\n",
    "mse_rf = mean_squared_error(y_test_ic50, y_pred_rf)\n",
    "r2_rf = r2_score(y_test_ic50, y_pred_rf)\n",
    "mae_rf = mean_absolute_error(y_test_ic50, y_pred_rf)\n",
    "\n",
    "print(\"\\nМетрики на тестовой выборке (Random Forest):\")\n",
    "print(f\"MSE: {mse_rf:.4f}\")\n",
    "print(f\"MAE: {mae_rf:.4f}\")\n",
    "print(f\"R2: {r2_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40e840a4-75ad-4bf4-b45d-359cc4c6b15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== LightGBM Регрессор для IC50 =====\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001142 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9262\n",
      "[LightGBM] [Info] Number of data points in the train set: 773, number of used features: 62\n",
      "[LightGBM] [Info] Start training from score 1.653887\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Лучшие параметры для LightGBM: {'regressor__learning_rate': 0.01, 'regressor__n_estimators': 300, 'regressor__num_leaves': 31, 'regressor__reg_alpha': 0.5, 'regressor__reg_lambda': 0.1}\n",
      "Лучший R2 на кросс-валидации: 0.42526789798837805\n",
      "\n",
      "Метрики на тестовой выборке (LightGBM):\n",
      "MSE: 0.5112\n",
      "MAE: 0.5697\n",
      "R2: 0.3758\n"
     ]
    }
   ],
   "source": [
    "# Модель 3: LightGBM Регрессор \n",
    "print(\"\\n===== LightGBM Регрессор для IC50 =====\")\n",
    "pipeline_lgbm = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LGBMRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Гиперпараметры для настройки LightGBM\n",
    "param_grid_lgbm = {\n",
    "    'regressor__n_estimators': [100, 200, 300],\n",
    "    'regressor__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'regressor__num_leaves': [31, 63],\n",
    "    'regressor__reg_alpha': [0.1, 0.5],\n",
    "    'regressor__reg_lambda': [0.1, 0.5]\n",
    "}\n",
    "\n",
    "grid_search_lgbm = GridSearchCV(\n",
    "    pipeline_lgbm,\n",
    "    param_grid_lgbm,\n",
    "    cv=5,\n",
    "    scoring=scoring_metrics,\n",
    "    refit='R2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_lgbm.fit(X_train_ic50, y_train_ic50)\n",
    "\n",
    "print(\"Лучшие параметры для LightGBM:\", grid_search_lgbm.best_params_)\n",
    "print(\"Лучший R2 на кросс-валидации:\", grid_search_lgbm.best_score_)\n",
    "\n",
    "# Оценка на тестовой выборке\n",
    "y_pred_lgbm = grid_search_lgbm.predict(X_test_ic50)\n",
    "mse_lgbm = mean_squared_error(y_test_ic50, y_pred_lgbm)\n",
    "r2_lgbm = r2_score(y_test_ic50, y_pred_lgbm)\n",
    "mae_lgbm = mean_absolute_error(y_test_ic50, y_pred_lgbm)\n",
    "\n",
    "print(\"\\nМетрики на тестовой выборке (LightGBM):\")\n",
    "print(f\"MSE: {mse_lgbm:.4f}\")\n",
    "print(f\"MAE: {mae_lgbm:.4f}\")\n",
    "print(f\"R2: {r2_lgbm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0f84491-55b9-4cab-ad98-93041d87befb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== XGBoost Регрессор для IC50 =====\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Лучшие параметры для XGBoost: {'regressor__colsample_bytree': 0.7, 'regressor__learning_rate': 0.05, 'regressor__max_depth': 3, 'regressor__n_estimators': 200, 'regressor__subsample': 0.7}\n",
      "Лучший R2 на кросс-валидации: 0.44381606754397246\n",
      "\n",
      "Метрики на тестовой выборке (XGBoost):\n",
      "MSE: 0.4456\n",
      "MAE: 0.5267\n",
      "R2: 0.4559\n"
     ]
    }
   ],
   "source": [
    "# Модель 4: XGBoost Regressor\n",
    "print(\"\\n===== XGBoost Регрессор для IC50 =====\")\n",
    "\n",
    "pipeline_xgb = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', XGBRegressor(random_state=42, eval_metric='rmse')) # eval_metric для предотвращения предупреждений\n",
    "])\n",
    "\n",
    "# Гиперпараметры для настройки XGBoost\n",
    "param_grid_xgb = {\n",
    "    'regressor__n_estimators': [100, 200, 300],\n",
    "    'regressor__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'regressor__max_depth': [3, 5, 7],\n",
    "    'regressor__subsample': [0.7, 1.0], # Доля выборок для обучения деревьев\n",
    "    'regressor__colsample_bytree': [0.7, 1.0] # Доля признаков для каждого дерева\n",
    "}\n",
    "\n",
    "grid_search_xgb = GridSearchCV(\n",
    "    pipeline_xgb,\n",
    "    param_grid_xgb,\n",
    "    cv=5,\n",
    "    scoring=scoring_metrics,\n",
    "    refit='R2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_xgb.fit(X_train_ic50, y_train_ic50)\n",
    "\n",
    "print(\"Лучшие параметры для XGBoost:\", grid_search_xgb.best_params_)\n",
    "print(\"Лучший R2 на кросс-валидации:\", grid_search_xgb.best_score_)\n",
    "\n",
    "# Оценка на тестовой выборке\n",
    "y_pred_xgb = grid_search_xgb.predict(X_test_ic50)\n",
    "mse_xgb = mean_squared_error(y_test_ic50, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test_ic50, y_pred_xgb)\n",
    "mae_xgb = mean_absolute_error(y_test_ic50, y_pred_xgb)\n",
    "\n",
    "print(\"\\nМетрики на тестовой выборке (XGBoost):\")\n",
    "print(f\"MSE: {mse_xgb:.4f}\")\n",
    "print(f\"MAE: {mae_xgb:.4f}\")\n",
    "print(f\"R2: {r2_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8ad0980-9a85-42e6-a32c-6f951ac9d431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== CatBoost Регрессор для IC50 =====\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "Лучшие параметры для CatBoost: {'regressor__depth': 6, 'regressor__iterations': 200, 'regressor__l2_leaf_reg': 1, 'regressor__learning_rate': 0.05}\n",
      "Лучший R2 на кросс-валидации: 0.45300200040076816\n",
      "\n",
      "Метрики на тестовой выборке (CatBoost):\n",
      "MSE: 0.4696\n",
      "MAE: 0.5395\n",
      "R2: 0.4266\n"
     ]
    }
   ],
   "source": [
    "# Модель 5: CatBoost Regressor\n",
    "\n",
    "print(\"\\n===== CatBoost Регрессор для IC50 =====\")\n",
    "\n",
    "pipeline_cat = Pipeline([\n",
    "    ('scaler', StandardScaler()), # CatBoost менее чувствителен к масштабированию, но для пайплайна оставляем\n",
    "    ('regressor', CatBoostRegressor(random_state=42, verbose=0)) # verbose=0 отключает вывод логов CatBoost\n",
    "])\n",
    "\n",
    "# Гиперпараметры для настройки CatBoost\n",
    "param_grid_cat = {\n",
    "    'regressor__iterations': [100, 200, 300], # Количество итераций (деревьев)\n",
    "    'regressor__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'regressor__depth': [4, 6, 8], # Глубина деревьев\n",
    "    'regressor__l2_leaf_reg': [1, 3, 5] # L2 регуляризация\n",
    "}\n",
    "\n",
    "grid_search_cat = GridSearchCV(\n",
    "    pipeline_cat,\n",
    "    param_grid_cat,\n",
    "    cv=5,\n",
    "    scoring=scoring_metrics,\n",
    "    refit='R2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_cat.fit(X_train_ic50, y_train_ic50)\n",
    "\n",
    "print(\"Лучшие параметры для CatBoost:\", grid_search_cat.best_params_)\n",
    "print(\"Лучший R2 на кросс-валидации:\", grid_search_cat.best_score_)\n",
    "\n",
    "# Оценка на тестовой выборке\n",
    "y_pred_cat = grid_search_cat.predict(X_test_ic50)\n",
    "mse_cat = mean_squared_error(y_test_ic50, y_pred_cat)\n",
    "r2_cat = r2_score(y_test_ic50, y_pred_cat)\n",
    "mae_cat = mean_absolute_error(y_test_ic50, y_pred_cat)\n",
    "\n",
    "print(\"\\nМетрики на тестовой выборке (CatBoost):\")\n",
    "print(f\"MSE: {mse_cat:.4f}\")\n",
    "print(f\"MAE: {mae_cat:.4f}\")\n",
    "print(f\"R2: {r2_cat:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "324ba558-d0a2-4091-9163-2f3d036d2cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Простая Нейронная Сеть для IC50 =====\n",
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры для Нейронной Сети: {'regressor__activation': 'relu', 'regressor__batch_size': 64, 'regressor__epochs': 50, 'regressor__hidden_layers': 2, 'regressor__learning_rate': 0.001, 'regressor__neurons': 32, 'regressor__optimizer': 'adam'}\n",
      "Лучший R2 на кросс-валидации: 0.3641236628320949\n",
      "\n",
      "Метрики на тестовой выборке (Нейронная Сеть):\n",
      "MSE: 0.5917\n",
      "MAE: 0.5947\n",
      "R2: 0.2775\n"
     ]
    }
   ],
   "source": [
    "# Модель 6: Простая Нейронная Сеть (Keras Sequential)\n",
    "print(\"\\n===== Простая Нейронная Сеть для IC50 =====\")\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers as L\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "# Функция для создания Keras-модели\n",
    "# Параметры, которые мы хотим усовершенствовать через GridSearchCV, должны быть аргументами этой функции.\n",
    "def build_nn_model(meta, hidden_layers=1, neurons=32, activation='relu',\n",
    "                   optimizer='adam', learning_rate=0.001):\n",
    "    n_features = meta[\"n_features_in_\"]\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(L.Input(shape=(n_features,)))\n",
    "    \n",
    "    for _ in range(hidden_layers):\n",
    "        model.add(L.Dense(neurons, activation=activation))\n",
    "        \n",
    "    model.add(L.Dense(1)) # Выходной слой для регрессии (1 нейрон, без активации)\n",
    "    \n",
    "    # Создаем экземпляр оптимизатора с указанной скоростью обучения\n",
    "    if optimizer == 'adam':\n",
    "        opt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        opt = keras.optimizers.Adam(learning_rate=learning_rate) # По умолчанию Adam\n",
    "\n",
    "    model.compile(optimizer=opt, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Здесь мы указываем KerasRegressor\n",
    "pipeline_nn = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', KerasRegressor(\n",
    "        model=build_nn_model,\n",
    "        # Задаем дефолтные значения для KerasRegressor, они будут переопределяться GridSearchCV\n",
    "        # Важно: эти параметры должны быть аргументами build_nn_model\n",
    "        hidden_layers=1,\n",
    "        neurons=32,\n",
    "        activation='relu',\n",
    "        optimizer='adam',\n",
    "        learning_rate=0.001,\n",
    "        batch_size=32, # batch_size и epochs - это параметры .fit(), а не .build_model()\n",
    "        epochs=50,\n",
    "        verbose=0,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Гиперпараметры для настройки Нейронной Сети\n",
    "# Параметры указываются с префиксом 'regressor__' для шага 'regressor' в пайплайне\n",
    "# Эти параметры будут переданы в конструктор KerasRegressor, который затем передаст их в build_nn_model.\n",
    "param_grid_nn = {\n",
    "    'regressor__hidden_layers': [1, 2],\n",
    "    'regressor__neurons': [32, 64],\n",
    "    'regressor__activation': ['relu'],\n",
    "    'regressor__optimizer': ['adam'],\n",
    "    'regressor__learning_rate': [0.001, 0.01],\n",
    "    'regressor__batch_size': [32, 64],\n",
    "    'regressor__epochs': [50, 100]\n",
    "}\n",
    "\n",
    "grid_search_nn = GridSearchCV(\n",
    "    pipeline_nn,\n",
    "    param_grid_nn,\n",
    "    cv=3,\n",
    "    scoring=scoring_metrics,\n",
    "    refit='R2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_nn.fit(X_train_ic50, y_train_ic50)\n",
    "\n",
    "print(\"Лучшие параметры для Нейронной Сети:\", grid_search_nn.best_params_)\n",
    "print(\"Лучший R2 на кросс-валидации:\", grid_search_nn.best_score_)\n",
    "\n",
    "# Оценка на тестовой выборке\n",
    "y_pred_nn = grid_search_nn.predict(X_test_ic50)\n",
    "mse_nn = mean_squared_error(y_test_ic50, y_pred_nn)\n",
    "r2_nn = r2_score(y_test_ic50, y_pred_nn)\n",
    "mae_nn = mean_absolute_error(y_test_ic50, y_pred_nn)\n",
    "\n",
    "print(\"\\nМетрики на тестовой выборке (Нейронная Сеть):\")\n",
    "print(f\"MSE: {mse_nn:.4f}\")\n",
    "print(f\"MAE: {mae_nn:.4f}\")\n",
    "print(f\"R2: {r2_nn:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ec5fc2-521d-4deb-90a1-303a472fd22f",
   "metadata": {},
   "source": [
    "Анализ результатов регрессии для log_IC50\n",
    "\n",
    "1. Линейная Регрессия\n",
    "R2 на кросс-валидации: 0.2813\n",
    "Метрики на тестовой выборке: MSE: 0.5935, MAE: 0.6009, R2: 0.2753\n",
    "Модель линейной регрессии продемонстрировала наиболее низкие показатели R2 среди всех протестированных моделей, как на кросс-валидации, так и на тестовой выборке. Значение R2 на уровне 0.275 на тестовой выборке указывает на то, что линейная модель объясняет лишь около 27.5% дисперсии целевой переменной. Это свидетельствует о наличии значительных нелинейных зависимостей в данных, которые не могут быть адекватно захвачены простым линейным подходом.\n",
    "\n",
    "2. Random Forest Регрессор\n",
    "Лучшие параметры: {'regressor__max_features': 0.6, 'regressor__min_samples_leaf': 5, 'regressor__n_estimators': 200}\n",
    "R2 на кросс-валидации: 0.4293\n",
    "Метрики на тестовой выборке: MSE: 0.4616, MAE: 0.5325, R2: 0.4364\n",
    "Модель Random Forest показала значительно лучшие результаты по сравнению с линейной регрессией. Значение R2 на тестовой выборке составило 0.436, что означает объяснение более 43% дисперсии log_IC50. Это демонстрирует способность ансамблевых методов на основе деревьев эффективно моделировать нелинейные отношения в данных. Показатели MAE также снизились, указывая на повышение точности предсказаний.\n",
    "\n",
    "3. LightGBM Регрессор\n",
    "Лучшие параметры: {'regressor__learning_rate': 0.01, 'regressor__n_estimators': 300, 'regressor__num_leaves': 31, 'regressor__reg_alpha': 0.5, 'regressor__reg_lambda': 0.1}\n",
    "R2 на кросс-валидации: 0.4253\n",
    "Метрики на тестовой выборке: MSE: 0.5112, MAE: 0.5697, R2: 0.3758\n",
    "LightGBM продемонстрировал хорошие результаты, однако уступил Random Forest по показателю R2 на тестовой выборке, достигнув значения 0.376. Расхождение между R2 на кросс-валидации и тестовой выборке (0.425 против 0.376) может указывать на некоторое переобучение модели или её меньшую обобщающую способность на новых, ранее не виденных данных, по сравнению с другими ансамблевыми методами.\n",
    "\n",
    "4. XGBoost Регрессор\n",
    "Лучшие параметры: {'regressor__colsample_bytree': 0.7, 'regressor__learning_rate': 0.05, 'regressor__max_depth': 3, 'regressor__n_estimators': 200, 'regressor__subsample': 0.7}\n",
    "R2 на кросс-валидации: 0.4438\n",
    "Метрики на тестовой выборке: MSE: 0.4456, MAE: 0.5267, R2: 0.4559\n",
    "XGBoost показал наилучшие результаты среди всех протестированных моделей для прогнозирования log_IC50. Его R2 на тестовой выборке составил 0.456, что является самым высоким показателем и свидетельствует об объяснении почти 45.6% дисперсии целевой переменной. Значение MAE также является одним из самых низких (0.527), что указывает на высокую точность предсказаний.\n",
    "\n",
    "5. CatBoost Регрессор\n",
    "Лучшие параметры: {'regressor__depth': 6, 'regressor__iterations': 200, 'regressor__l2_leaf_reg': 1, 'regressor__learning_rate': 0.05}\n",
    "R2 на кросс-валидации: 0.4530\n",
    "Метрики на тестовой выборке: MSE: 0.4696, MAE: 0.5395, R2: 0.4266\n",
    "CatBoost также продемонстрировал высокие результаты, но на тестовой выборке незначительно уступил XGBoost, достигнув R2 в 0.427. Несмотря на более высокий R2 на кросс-валидации (0.453), производительность на независимой тестовой выборке оказалась чуть ниже, чем у XGBoost, что является типичным для оценок обобщающей способности модели.\n",
    "\n",
    "6. Простая Нейронная Сеть\n",
    "Лучшие параметры: {'regressor__activation': 'relu', 'regressor__batch_size': 64, 'regressor__epochs': 50, 'regressor__hidden_layers': 2, 'regressor__learning_rate': 0.001, 'regressor__neurons': 32, 'regressor__optimizer': 'adam'}\n",
    "R2 на кросс-валидации: 0.3641\n",
    "Метрики на тестовой выборке: MSE: 0.5917, MAE: 0.5947, R2: 0.2775\n",
    "Простая нейронная сеть показала результаты, сравнимые с линейной регрессией, с R2 на тестовой выборке 0.277. Это указывает на то, что данная архитектура нейронной сети недостаточно сложна или обучена для эффективного улавливания сложных закономерностей в данных, необходимых для точного прогнозирования log_IC50.\n",
    "\n",
    "### Общий вывод по прогнозированию log_IC50:\n",
    "Для задачи прогнозирования log_IC50 XGBoost Регрессор является наиболее эффективной моделью, демонстрируя самый высокий показатель R2 на тестовой выборке (0.456). Модели Random Forest и CatBoost также показали сильные результаты, подтверждая, что ансамблевые методы на основе деревьев решений хорошо подходят для данной задачи. В то же время, линейная регрессия и простая нейронная сеть оказались менее эффективными. Полученные результаты по log_IC50 значительно лучше, чем для log_SI, что указывает на лучшую объяснительную способность текущего набора признаков для этой целевой переменной"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8029736c-723a-4b28-9e24-f5908066bacc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
