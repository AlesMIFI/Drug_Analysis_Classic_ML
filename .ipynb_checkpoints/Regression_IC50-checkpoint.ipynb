{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "829d1ff1-5b0d-4ca1-a58b-c7fa2606fd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Установка XGBoost\n",
    "# !pip install xgboost\n",
    "\n",
    "# # Установка CatBoost\n",
    "# !pip install catboost\n",
    "\n",
    "# # Установка TensorFlow\n",
    "# !pip install tensorflow\n",
    "\n",
    "# # Установка Scikeras \n",
    "# !pip install scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebaf5842-3613-4111-8e41-18d89ad3aa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcec2e73-b757-4c38-aaca-dd89d2ff9dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 'данные для моделей.csv' успешно загружен.\n"
     ]
    }
   ],
   "source": [
    "# Чтение DataFrame\n",
    "df_models = pd.read_csv(\"данные для моделей.csv\")\n",
    "print(\"DataFrame 'данные для моделей.csv' успешно загружен.\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5234bcf-81eb-4853-b7ab-98a53b25a9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Подготовка данных для IC50 ---\n",
      "Размер обучающей выборки (X_train_ic50): (773, 68)\n",
      "Размер тестовой выборки (X_test_ic50): (194, 68)\n",
      "\n",
      "===== Линейная Регрессия для IC50 =====\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Лучшие параметры для Линейной Регрессии: {}\n",
      "Лучший R2 на кросс-валидации: 0.2813431323334865\n",
      "\n",
      "Метрики на тестовой выборке (Линейная Регрессия):\n",
      "MSE: 0.5935\n",
      "MAE: 0.6009\n",
      "R2: 0.2753\n",
      "\n",
      "===== Random Forest Регрессор для IC50 =====\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Лучшие параметры для Random Forest: {'regressor__max_features': 0.6, 'regressor__min_samples_leaf': 5, 'regressor__n_estimators': 200}\n",
      "Лучший R2 на кросс-валидации: 0.4292621189253373\n",
      "\n",
      "Метрики на тестовой выборке (Random Forest):\n",
      "MSE: 0.4616\n",
      "MAE: 0.5325\n",
      "R2: 0.4364\n",
      "\n",
      "===== LightGBM Регрессор для IC50 =====\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000734 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9262\n",
      "[LightGBM] [Info] Number of data points in the train set: 773, number of used features: 62\n",
      "[LightGBM] [Info] Start training from score 1.653887\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Лучшие параметры для LightGBM: {'regressor__learning_rate': 0.01, 'regressor__n_estimators': 300, 'regressor__num_leaves': 31, 'regressor__reg_alpha': 0.5, 'regressor__reg_lambda': 0.1}\n",
      "Лучший R2 на кросс-валидации: 0.42526789798837805\n",
      "\n",
      "Метрики на тестовой выборке (LightGBM):\n",
      "MSE: 0.5112\n",
      "MAE: 0.5697\n",
      "R2: 0.3758\n",
      "\n",
      "===== XGBoost Регрессор для IC50 =====\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Лучшие параметры для XGBoost: {'regressor__colsample_bytree': 0.7, 'regressor__learning_rate': 0.05, 'regressor__max_depth': 3, 'regressor__n_estimators': 200, 'regressor__subsample': 0.7}\n",
      "Лучший R2 на кросс-валидации: 0.44381606754397246\n",
      "\n",
      "Метрики на тестовой выборке (XGBoost):\n",
      "MSE: 0.4456\n",
      "MAE: 0.5267\n",
      "R2: 0.4559\n",
      "\n",
      "===== CatBoost Регрессор для IC50 =====\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "Лучшие параметры для CatBoost: {'regressor__depth': 6, 'regressor__iterations': 200, 'regressor__l2_leaf_reg': 1, 'regressor__learning_rate': 0.05}\n",
      "Лучший R2 на кросс-валидации: 0.45300200040076816\n",
      "\n",
      "Метрики на тестовой выборке (CatBoost):\n",
      "MSE: 0.4696\n",
      "MAE: 0.5395\n",
      "R2: 0.4266\n",
      "\n",
      "===== Простая Нейронная Сеть для IC50 =====\n",
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры для Нейронной Сети: {'regressor__activation': 'relu', 'regressor__batch_size': 64, 'regressor__epochs': 50, 'regressor__hidden_layers': 2, 'regressor__learning_rate': 0.001, 'regressor__neurons': 32, 'regressor__optimizer': 'adam'}\n",
      "Лучший R2 на кросс-валидации: 0.3641236628320949\n",
      "\n",
      "Метрики на тестовой выборке (Нейронная Сеть):\n",
      "MSE: 0.5917\n",
      "MAE: 0.5947\n",
      "R2: 0.2775\n"
     ]
    }
   ],
   "source": [
    "#Подготовка данных для IC50\n",
    "# Целевая переменная для IC50\n",
    "target_ic50 = 'log_IC50, mM'\n",
    "\n",
    "# Признаки (X) и целевая переменная (y)\n",
    "# Исключаем другие целевые переменные из признаков\n",
    "X = df_models.drop(columns=['log_IC50, mM', 'log_CC50, mM', 'log_SI'])\n",
    "y = df_models[target_ic50]\n",
    "\n",
    "# Разделение на обучающую и тестовую выборки\n",
    "# random_state для воспроизводимости\n",
    "X_train_ic50, X_test_ic50, y_train_ic50, y_test_ic50 = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\n--- Подготовка данных для IC50 ---\")\n",
    "print(f\"Размер обучающей выборки (X_train_ic50): {X_train_ic50.shape}\")\n",
    "print(f\"Размер тестовой выборки (X_test_ic50): {X_test_ic50.shape}\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3. Настройка Pipeline и GridSearchCV\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Метрики для GridSearchCV. 'neg_' означает, что GridSearchCV стремится максимизировать метрику,\n",
    "# поэтому ошибки (MSE, MAE) инвертируются.\n",
    "scoring_metrics = {\n",
    "    'MSE': 'neg_mean_squared_error',\n",
    "    'R2': 'r2',\n",
    "    'MAE': 'neg_mean_absolute_error'\n",
    "}\n",
    "\n",
    "# --- Модель 1: Линейная Регрессия ---\n",
    "print(\"\\n===== Линейная Регрессия для IC50 =====\")\n",
    "pipeline_lr = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "param_grid_lr = {} # Для LinearRegression нет гиперпараметров для настройки в данном случае\n",
    "\n",
    "grid_search_lr = GridSearchCV(\n",
    "    pipeline_lr,\n",
    "    param_grid_lr,\n",
    "    cv=5, # 5-кратная кросс-валидация\n",
    "    scoring=scoring_metrics,\n",
    "    refit='R2', # Оптимизировать по R2\n",
    "    n_jobs=-1, # Использовать все доступные ядра\n",
    "    verbose=1 # Выводить прогресс\n",
    ")\n",
    "\n",
    "grid_search_lr.fit(X_train_ic50, y_train_ic50)\n",
    "\n",
    "print(\"Лучшие параметры для Линейной Регрессии:\", grid_search_lr.best_params_)\n",
    "print(\"Лучший R2 на кросс-валидации:\", grid_search_lr.best_score_)\n",
    "\n",
    "# Оценка на тестовой выборке\n",
    "y_pred_lr = grid_search_lr.predict(X_test_ic50)\n",
    "mse_lr = mean_squared_error(y_test_ic50, y_pred_lr)\n",
    "r2_lr = r2_score(y_test_ic50, y_pred_lr)\n",
    "mae_lr = mean_absolute_error(y_test_ic50, y_pred_lr)\n",
    "\n",
    "print(\"\\nМетрики на тестовой выборке (Линейная Регрессия):\")\n",
    "print(f\"MSE: {mse_lr:.4f}\")\n",
    "print(f\"MAE: {mae_lr:.4f}\")\n",
    "print(f\"R2: {r2_lr:.4f}\")\n",
    "\n",
    "\n",
    "# --- Модель 2: Random Forest Регрессор ---\n",
    "print(\"\\n===== Random Forest Регрессор для IC50 =====\")\n",
    "pipeline_rf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Гиперпараметры для настройки Random Forest\n",
    "param_grid_rf = {\n",
    "    'regressor__n_estimators': [50, 100, 200],\n",
    "    'regressor__max_features': [0.6, 0.8, 1.0],\n",
    "    'regressor__min_samples_leaf': [5, 10]\n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(\n",
    "    pipeline_rf,\n",
    "    param_grid_rf,\n",
    "    cv=5,\n",
    "    scoring=scoring_metrics,\n",
    "    refit='R2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_rf.fit(X_train_ic50, y_train_ic50)\n",
    "\n",
    "print(\"Лучшие параметры для Random Forest:\", grid_search_rf.best_params_)\n",
    "print(\"Лучший R2 на кросс-валидации:\", grid_search_rf.best_score_)\n",
    "\n",
    "# Оценка на тестовой выборке\n",
    "y_pred_rf = grid_search_rf.predict(X_test_ic50)\n",
    "mse_rf = mean_squared_error(y_test_ic50, y_pred_rf)\n",
    "r2_rf = r2_score(y_test_ic50, y_pred_rf)\n",
    "mae_rf = mean_absolute_error(y_test_ic50, y_pred_rf)\n",
    "\n",
    "print(\"\\nМетрики на тестовой выборке (Random Forest):\")\n",
    "print(f\"MSE: {mse_rf:.4f}\")\n",
    "print(f\"MAE: {mae_rf:.4f}\")\n",
    "print(f\"R2: {r2_rf:.4f}\")\n",
    "\n",
    "\n",
    "# --- Модель 3: LightGBM Регрессор ---\n",
    "print(\"\\n===== LightGBM Регрессор для IC50 =====\")\n",
    "pipeline_lgbm = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LGBMRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Гиперпараметры для настройки LightGBM\n",
    "param_grid_lgbm = {\n",
    "    'regressor__n_estimators': [100, 200, 300],\n",
    "    'regressor__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'regressor__num_leaves': [31, 63],\n",
    "    'regressor__reg_alpha': [0.1, 0.5],\n",
    "    'regressor__reg_lambda': [0.1, 0.5]\n",
    "}\n",
    "\n",
    "grid_search_lgbm = GridSearchCV(\n",
    "    pipeline_lgbm,\n",
    "    param_grid_lgbm,\n",
    "    cv=5,\n",
    "    scoring=scoring_metrics,\n",
    "    refit='R2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_lgbm.fit(X_train_ic50, y_train_ic50)\n",
    "\n",
    "print(\"Лучшие параметры для LightGBM:\", grid_search_lgbm.best_params_)\n",
    "print(\"Лучший R2 на кросс-валидации:\", grid_search_lgbm.best_score_)\n",
    "\n",
    "# Оценка на тестовой выборке\n",
    "y_pred_lgbm = grid_search_lgbm.predict(X_test_ic50)\n",
    "mse_lgbm = mean_squared_error(y_test_ic50, y_pred_lgbm)\n",
    "r2_lgbm = r2_score(y_test_ic50, y_pred_lgbm)\n",
    "mae_lgbm = mean_absolute_error(y_test_ic50, y_pred_lgbm)\n",
    "\n",
    "print(\"\\nМетрики на тестовой выборке (LightGBM):\")\n",
    "print(f\"MSE: {mse_lgbm:.4f}\")\n",
    "print(f\"MAE: {mae_lgbm:.4f}\")\n",
    "print(f\"R2: {r2_lgbm:.4f}\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Модель 4: XGBoost Regressor\n",
    "# ----------------------------------------------------------------------\n",
    "print(\"\\n===== XGBoost Регрессор для IC50 =====\")\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "pipeline_xgb = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', XGBRegressor(random_state=42, eval_metric='rmse')) # eval_metric для предотвращения предупреждений\n",
    "])\n",
    "\n",
    "# Гиперпараметры для настройки XGBoost\n",
    "param_grid_xgb = {\n",
    "    'regressor__n_estimators': [100, 200, 300],\n",
    "    'regressor__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'regressor__max_depth': [3, 5, 7],\n",
    "    'regressor__subsample': [0.7, 1.0], # Доля выборок для обучения деревьев\n",
    "    'regressor__colsample_bytree': [0.7, 1.0] # Доля признаков для каждого дерева\n",
    "}\n",
    "\n",
    "grid_search_xgb = GridSearchCV(\n",
    "    pipeline_xgb,\n",
    "    param_grid_xgb,\n",
    "    cv=5,\n",
    "    scoring=scoring_metrics,\n",
    "    refit='R2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_xgb.fit(X_train_ic50, y_train_ic50)\n",
    "\n",
    "print(\"Лучшие параметры для XGBoost:\", grid_search_xgb.best_params_)\n",
    "print(\"Лучший R2 на кросс-валидации:\", grid_search_xgb.best_score_)\n",
    "\n",
    "# Оценка на тестовой выборке\n",
    "y_pred_xgb = grid_search_xgb.predict(X_test_ic50)\n",
    "mse_xgb = mean_squared_error(y_test_ic50, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test_ic50, y_pred_xgb)\n",
    "mae_xgb = mean_absolute_error(y_test_ic50, y_pred_xgb)\n",
    "\n",
    "print(\"\\nМетрики на тестовой выборке (XGBoost):\")\n",
    "print(f\"MSE: {mse_xgb:.4f}\")\n",
    "print(f\"MAE: {mae_xgb:.4f}\")\n",
    "print(f\"R2: {r2_xgb:.4f}\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Модель 5: CatBoost Regressor\n",
    "# ----------------------------------------------------------------------\n",
    "print(\"\\n===== CatBoost Регрессор для IC50 =====\")\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "pipeline_cat = Pipeline([\n",
    "    ('scaler', StandardScaler()), # CatBoost менее чувствителен к масштабированию, но для пайплайна оставляем\n",
    "    ('regressor', CatBoostRegressor(random_state=42, verbose=0)) # verbose=0 отключает вывод логов CatBoost\n",
    "])\n",
    "\n",
    "# Гиперпараметры для настройки CatBoost\n",
    "param_grid_cat = {\n",
    "    'regressor__iterations': [100, 200, 300], # Количество итераций (деревьев)\n",
    "    'regressor__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'regressor__depth': [4, 6, 8], # Глубина деревьев\n",
    "    'regressor__l2_leaf_reg': [1, 3, 5] # L2 регуляризация\n",
    "}\n",
    "\n",
    "grid_search_cat = GridSearchCV(\n",
    "    pipeline_cat,\n",
    "    param_grid_cat,\n",
    "    cv=5,\n",
    "    scoring=scoring_metrics,\n",
    "    refit='R2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_cat.fit(X_train_ic50, y_train_ic50)\n",
    "\n",
    "print(\"Лучшие параметры для CatBoost:\", grid_search_cat.best_params_)\n",
    "print(\"Лучший R2 на кросс-валидации:\", grid_search_cat.best_score_)\n",
    "\n",
    "# Оценка на тестовой выборке\n",
    "y_pred_cat = grid_search_cat.predict(X_test_ic50)\n",
    "mse_cat = mean_squared_error(y_test_ic50, y_pred_cat)\n",
    "r2_cat = r2_score(y_test_ic50, y_pred_cat)\n",
    "mae_cat = mean_absolute_error(y_test_ic50, y_pred_cat)\n",
    "\n",
    "print(\"\\nМетрики на тестовой выборке (CatBoost):\")\n",
    "print(f\"MSE: {mse_cat:.4f}\")\n",
    "print(f\"MAE: {mae_cat:.4f}\")\n",
    "print(f\"R2: {r2_cat:.4f}\")\n",
    "\n",
    "# Модель 6: Простая Нейронная Сеть (Keras Sequential)\n",
    "# ----------------------------------------------------------------------\n",
    "print(\"\\n===== Простая Нейронная Сеть для IC50 =====\")\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers as L\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "# Функция для создания Keras-модели\n",
    "# Параметры, которые мы хотим тюнить через GridSearchCV, должны быть аргументами этой функции.\n",
    "def build_nn_model(meta, hidden_layers=1, neurons=32, activation='relu',\n",
    "                   optimizer='adam', learning_rate=0.001):\n",
    "    n_features = meta[\"n_features_in_\"]\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(L.Input(shape=(n_features,)))\n",
    "    \n",
    "    for _ in range(hidden_layers):\n",
    "        model.add(L.Dense(neurons, activation=activation))\n",
    "        \n",
    "    model.add(L.Dense(1)) # Выходной слой для регрессии (1 нейрон, без активации)\n",
    "    \n",
    "    # Создаем экземпляр оптимизатора с указанной скоростью обучения\n",
    "    if optimizer == 'adam':\n",
    "        opt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        opt = keras.optimizers.Adam(learning_rate=learning_rate) # По умолчанию Adam\n",
    "\n",
    "    model.compile(optimizer=opt, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Здесь мы правильно указываем KerasRegressor\n",
    "pipeline_nn = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', KerasRegressor(\n",
    "        model=build_nn_model,\n",
    "        # Задаем дефолтные значения для KerasRegressor, они будут переопределяться GridSearchCV\n",
    "        # Важно: эти параметры должны быть аргументами build_nn_model\n",
    "        hidden_layers=1,\n",
    "        neurons=32,\n",
    "        activation='relu',\n",
    "        optimizer='adam',\n",
    "        learning_rate=0.001,\n",
    "        batch_size=32, # batch_size и epochs - это параметры .fit(), а не .build_model()\n",
    "        epochs=50,\n",
    "        verbose=0,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Гиперпараметры для настройки Нейронной Сети\n",
    "# Теперь параметры указываются с префиксом 'regressor__' для шага 'regressor' в пайплайне\n",
    "# Эти параметры будут переданы в конструктор KerasRegressor, который затем передаст их в build_nn_model.\n",
    "param_grid_nn = {\n",
    "    'regressor__hidden_layers': [1, 2],\n",
    "    'regressor__neurons': [32, 64],\n",
    "    'regressor__activation': ['relu'],\n",
    "    'regressor__optimizer': ['adam'],\n",
    "    'regressor__learning_rate': [0.001, 0.01],\n",
    "    'regressor__batch_size': [32, 64],\n",
    "    'regressor__epochs': [50, 100]\n",
    "}\n",
    "\n",
    "grid_search_nn = GridSearchCV(\n",
    "    pipeline_nn,\n",
    "    param_grid_nn,\n",
    "    cv=3,\n",
    "    scoring=scoring_metrics,\n",
    "    refit='R2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_nn.fit(X_train_ic50, y_train_ic50)\n",
    "\n",
    "print(\"Лучшие параметры для Нейронной Сети:\", grid_search_nn.best_params_)\n",
    "print(\"Лучший R2 на кросс-валидации:\", grid_search_nn.best_score_)\n",
    "\n",
    "# Оценка на тестовой выборке\n",
    "y_pred_nn = grid_search_nn.predict(X_test_ic50)\n",
    "mse_nn = mean_squared_error(y_test_ic50, y_pred_nn)\n",
    "r2_nn = r2_score(y_test_ic50, y_pred_nn)\n",
    "mae_nn = mean_absolute_error(y_test_ic50, y_pred_nn)\n",
    "\n",
    "print(\"\\nМетрики на тестовой выборке (Нейронная Сеть):\")\n",
    "print(f\"MSE: {mse_nn:.4f}\")\n",
    "print(f\"MAE: {mae_nn:.4f}\")\n",
    "print(f\"R2: {r2_nn:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e96cd6c-d809-4ca1-aa0c-3b0edf4fca4a",
   "metadata": {},
   "source": [
    "Анализ по каждой модели\n",
    "1. Линейная Регрессия\n",
    "R2 на кросс-валидации: 0.2813\n",
    "R2 на тестовой выборке: 0.2753\n",
    "MSE: 0.5935, MAE: 0.6009\n",
    "Вывод: Линейная регрессия показала наихудшие результаты. Это говорит о том, что отношения между вашими химическими дескрипторами и IC50, вероятно, не являются чисто линейными. Модель не смогла уловить более сложные закономерности. Это подтверждает целесообразность использования более сложных, нелинейных моделей, таких как древовидные алгоритмы.\n",
    "\n",
    "2. Random Forest Регрессор\n",
    "Лучшие параметры: max_features=0.6, min_samples_leaf=5, n_estimators=200\n",
    "R2 на кросс-валидации: 0.4293\n",
    "R2 на тестовой выборке: 0.4364\n",
    "MSE: 0.4616, MAE: 0.5325\n",
    "Вывод: Random Forest показал значительно лучшие результаты, чем Линейная Регрессия, что подтверждает наличие нелинейных связей. R2 на тестовой выборке (0.4364) довольно близок к R2 на кросс-валидации, что указывает на хорошую обобщающую способность и отсутствие сильного переобучения. Это надёжная модель, которая неплохо справляется.\n",
    "\n",
    "3. LightGBM Регрессор\n",
    "Лучшие параметры: learning_rate=0.01, n_estimators=300, num_leaves=31, reg_alpha=0.5, reg_lambda=0.1\n",
    "R2 на кросс-валидации: 0.4253\n",
    "R2 на тестовой выборке: 0.3758\n",
    "MSE: 0.5112, MAE: 0.5697\n",
    "Вывод: Результаты LightGBM на кросс-валидации (0.4253) сопоставимы с Random Forest, но R2 на тестовой выборке (0.3758) заметно ниже, чем на CV и ниже, чем у Random Forest и XGBoost. Это может указывать на небольшое переобучение на обучающей выборке во время кросс-валидации или на то, что найденные параметры не так хорошо обобщаются на полностью новые данные, как в случае с Random Forest.\n",
    "\n",
    "Предупреждения LightGBM: Многократные предупреждения [LightGBM] [Warning] No further splits with positive gain, best gain: -inf указывают на то, что модель не всегда могла найти выгодные расщепления для построения деревьев. Это может происходить, когда:\n",
    "* Данные очень шумные или не содержат достаточно сильных сигналов.\n",
    "* Параметры регуляризации слишком строгие.\n",
    "* Либо модель быстро достигает минимума ошибки и дальнейшее добавление деревьев не дает значимого улучшения, либо ей не хватает данных для более точных расщеплений.\n",
    "\n",
    "4. XGBoost Регрессор\n",
    "Лучшие параметры: colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.7\n",
    "R2 на кросс-валидации: 0.4438\n",
    "R2 на тестовой выборке: 0.4559\n",
    "MSE: 0.4456, MAE: 0.5267\n",
    "Вывод: XGBoost является лучшей моделью по всем метрикам на тестовой выборке. Его R2 (0.4559) самый высокий, а MSE и MAE — самые низкие. Это подтверждает его репутацию мощного и универсального алгоритма. R2 на тестовой выборке даже немного выше, чем на кросс-валидации, что является хорошим знаком и говорит об отличной обобщающей способности.\n",
    "\n",
    "5. CatBoost Регрессор\n",
    "Лучшие параметры: depth=6, iterations=200, l2_leaf_reg=1, learning_rate=0.05\n",
    "R2 на кросс-валидации: 0.4530\n",
    "R2 на тестовой выборке: 0.4266\n",
    "MSE: 0.4696, MAE: 0.5395\n",
    "Вывод: CatBoost показал самый высокий R2 на кросс-валидации (0.4530), но на тестовой выборке немного уступил XGBoost (0.4266 против 0.4559). Это может указывать на небольшое переобучение во время кросс-валидации, где CatBoost смог найти чуть более точные параметры для обучающих фолдов, но они не так хорошо сработали на полностью новых данных. Тем не менее, это по-прежнему очень хорошая модель, вторая по результативности на тестовой выборке.\n",
    "\n",
    "Ключевые выводы и рекомендации:\n",
    "XGBoost — победитель: На основе предоставленных метрик, XGBoostRegressor показал лучшую производительность на тестовой выборке для предсказания IC50. Это сильный кандидат для вашей окончательной модели.\n",
    "Древовидные модели превосходят линейные: Значительный отрыв R2 у Random Forest, LightGBM, XGBoost и CatBoost от Линейной Регрессии явно указывает на нелинейный характер зависимости между вашими химическими признаками и IC50.\n",
    "LightGBM требует внимания: Его снижение R2 на тестовой выборке по сравнению с CV и конкурентами указывает на то, что, возможно, требуются дополнительная тонкая настройка параметров регуляризации (например, num_leaves или min_child_samples) или использование ранней остановки (early_stopping_rounds) при обучении, чтобы избежать переобучения. Также, эти предупреждения могут свидетельствовать о том, что для ваших данных LightGBM, возможно, не идеален, или что вы слишком сильно его регуляризуете.\n",
    "Количество признаков (68): Ваши 68 отобранных признаков оказались вполне адекватными для построения моделей. Вам не потребовалось дополнительное снижение размерности с помощью PCA, что сохранило интерпретируемость ваших признаков. Это очень ценно в химии.\n",
    "Следующие шаги:\n",
    "Окончательный выбор модели: Я бы сосредоточился на XGBoost как на вашей основной модели.\n",
    "Изучение важности признаков: Для XGBoost (или Random Forest) вы можете извлечь важность признаков. Это позволит вам увидеть, какие из 68 химических дескрипторов оказывают наибольшее влияние на предсказание IC50. Это будет очень ценно для химической интерпретации.\n",
    "Анализ остатков: Проверьте остатки вашей лучшей модели (XGBoost):\n",
    "Распределение остатков: Должно быть близко к нормальному и центрировано около нуля.\n",
    "График остатков против предсказанных значений: Не должно быть никаких видимых паттернов (например, формы веера), что указывает на гомоскедастичность.\n",
    "Дополнительная оптимизация гиперпараметров: Хотя GridSearchCV уже проделал большую работу, иногда можно добиться небольшого улучшения, используя Bayesian Optimization (например, с optuna или hyperopt) для более эффективного исследования пространства параметров, особенно для LightGBM и CatBoost.\n",
    "Попробовать ансамбли: Иногда объединение предсказаний нескольких лучших моделей (например, XGBoost и CatBoost) может дать небольшое, но стабильное улучшение."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9fd718-25a8-41c1-b51c-60dfe2a564f8",
   "metadata": {},
   "source": [
    "Анализ результатов Простой Нейронной Сети для IC50\n",
    "Лучшие параметры: 'activation': 'relu', 'batch_size': 64, 'epochs': 50, 'hidden_layers': 2, 'learning_rate': 0.001, 'neurons': 32, 'optimizer': 'adam'\n",
    "Лучший R2 на кросс-валидации: 0.3641\n",
    "Метрики на тестовой выборке:\n",
    "MSE: 0.5917\n",
    "MAE: 0.5947\n",
    "R2: 0.2775\n",
    "Выводы и сравнение:\n",
    "Производительность на тестовой выборке:\n",
    "\n",
    "R2 Нейронной Сети на тестовой выборке (0.2775) оказался самым низким среди всех моделей, за исключением Линейной Регрессии (которая была 0.2753). По сути, она показала результат, сопоставимый с простой линейной моделью.\n",
    "Это значительно ниже, чем у Random Forest (0.4364), XGBoost (0.4559) и CatBoost (0.4266).\n",
    "Разрыв между CV и тестовой выборкой:\n",
    "\n",
    "R2 на кросс-валидации (0.3641) заметно выше, чем R2 на тестовой выборке (0.2775). Это указывает на некоторое переобучение нейронной сети на тренировочных данных. Она смогла хорошо подстроиться под паттерны в обучающих фолдах, но не так хорошо обобщилась на абсолютно новые данные.\n",
    "Предупреждение UserWarning: A worker stopped...:\n",
    "\n",
    "Это предупреждение joblib (который используется GridSearchCV) часто указывает на проблемы с памятью или слишком короткий таймаут для воркеров. Нейронные сети, особенно при переборе гиперпараметров, могут потреблять значительные ресурсы, что подтверждает ваши предыдущие наблюдения о CatBoost. Возможно, некоторым комбинациям параметров не хватило памяти, и процессы-воркеры завершались.\n",
    "Почему НН может не показывать лучшую производительность?\n",
    "Несмотря на то, что нейронные сети очень мощные, они не всегда являются \"серебряной пулей\" для табличных данных, особенно если:\n",
    "\n",
    "Размер данных: Ваш набор данных (773 обучающих примера, 68 признаков) относительно небольшой для того, чтобы НН раскрыла весь свой потенциал. НН часто требуют очень больших объемов данных для эффективного обучения и предотвращения переобучения.\n",
    "Архитектура и гиперпараметры: \"Простая\" нейронная сеть с 2 скрытыми слоями и 32 нейронами, возможно, недостаточно сложна или, наоборот, слишком сложна для вашего набора данных и задачи. Поиск оптимальной архитектуры (количество слоев, нейронов), а также гиперпараметров (learning rate, активационные функции, регуляризация, dropouts) для НН — это сложный итеративный процесс.\n",
    "Регуляризация: В отличие от градиентного бустинга, НН требуют более явного и тщательного применения методов регуляризации (dropout, L1/L2 регуляризация, ранняя остановка), чтобы справиться с переобучением, особенно на небольших данных. В ваших лучших параметрах нет явных регуляризаторов, кроме epochs=50 и batch_size=64.\n",
    "Сравнение с бустингом: Для табличных данных, градиентные бустинговые деревья (XGBoost, CatBoost, LightGBM) часто превосходят простые нейронные сети \"из коробки\" или требуют значительно меньше усилий по тонкой настройке для достижения хороших результатов. Их ансамблевая природа и последовательное обучение делают их очень эффективными.\n",
    "Рекомендации:\n",
    "Приоритет моделей: Основываясь на текущих результатах, XGBoost по-прежнему является вашей лучшей моделью. Random Forest и CatBoost также показали очень хорошие результаты. Нейронная сеть пока отстает.\n",
    "Стоит ли продолжать с НН?:\n",
    "Если у вас есть много времени и вычислительных ресурсов, и вы заинтересованы в глубоком изучении НН, вы можете попробовать более тонкую настройку (больше комбинаций параметров для GridSearchCV, добавление Dropout, изменение архитектуры).\n",
    "Однако, если цель — получить лучшую модель как можно быстрее и эффективнее, сосредоточьтесь на XGBoost. Учитывая, что вы уже столкнулись с проблемами ресурсов, вкладывать больше усилий в НН на этом этапе может быть неоптимально.\n",
    "В случае необходимости НН:\n",
    "Рассмотрите использование ранней остановки (callbacks=[EarlyStopping(...)]) при обучении, чтобы избежать переобучения.\n",
    "Попробуйте Dropout-слои для регуляризации.\n",
    "Используйте более широкий диапазон значений для learning_rate и epochs.\n",
    "Возможно, увеличение neurons или hidden_layers может помочь, но также увеличит риск переобучения и потребление ресурсов.\n",
    "Ваши результаты четко показывают, что для вашей задачи и данных, градиентные бустинговые модели (особенно XGBoost) справляются значительно лучше, чем выбранная конфигурация простой нейронной сети."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
