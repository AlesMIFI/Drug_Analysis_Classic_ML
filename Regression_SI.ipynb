{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e52956c8-a936-4346-ac2f-8c0b7070841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers as L\n",
    "from scikeras.wrappers import KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecc3e001-5686-41db-8af3-80423f403fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 'данные для моделей.csv' успешно загружен.\n"
     ]
    }
   ],
   "source": [
    "# Чтение DataFrame\n",
    "df_models = pd.read_csv(\"данные для моделей.csv\")\n",
    "print(\"DataFrame 'данные для моделей.csv' успешно загружен.\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddf48466-a4a8-4c6b-9697-e104a5165ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Подготовка данных для SI ---\n",
      "Размер обучающей выборки (X_train_si): (773, 68)\n",
      "Размер тестовой выборки (X_test_si): (194, 68)\n"
     ]
    }
   ],
   "source": [
    "# Подготовка данных для SI\n",
    "# Целевая переменная для SI\n",
    "target_si = 'log_SI'\n",
    "\n",
    "# Признаки (X) и целевая переменная (y)\n",
    "# Исключаем другие целевые переменные из признаков\n",
    "X = df_models.drop(columns=['log_IC50, mM', 'log_CC50, mM', 'log_SI'])\n",
    "y = df_models[target_si]\n",
    "\n",
    "# Разделение на обучающую и тестовую выборки\n",
    "# random_state для воспроизводимости\n",
    "X_train_si, X_test_si, y_train_si, y_test_si = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\n--- Подготовка данных для SI ---\")\n",
    "print(f\"Размер обучающей выборки (X_train_si): {X_train_si.shape}\")\n",
    "print(f\"Размер тестовой выборки (X_test_si): {X_test_si.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "907315c2-36fc-477d-bae8-8e6845059c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Настройка Pipeline и GridSearchCV\n",
    "# 'neg_' означает, что GridSearchCV стремится максимизировать метрику,\n",
    "scoring_metrics = {\n",
    "    'MSE': 'neg_mean_squared_error',\n",
    "    'R2': 'r2',\n",
    "    'MAE': 'neg_mean_absolute_error'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb6a3619-d09f-43a4-b04a-520aba840314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Линейная Регрессия для SI =====\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Лучшие параметры для Линейной Регрессии: {}\n",
      "Лучший R2 на кросс-валидации: 0.06812373393945233\n",
      "\n",
      "Метрики на тестовой выборке (Линейная Регрессия):\n",
      "MSE: 0.5683\n",
      "MAE: 0.5716\n",
      "R2: 0.0015\n"
     ]
    }
   ],
   "source": [
    "# Модель 1: Линейная Регрессия \n",
    "print(\"\\n===== Линейная Регрессия для SI =====\")\n",
    "pipeline_lr = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "param_grid_lr = {} # Для LinearRegression нет гиперпараметров \n",
    "\n",
    "grid_search_lr = GridSearchCV(\n",
    "    pipeline_lr,\n",
    "    param_grid_lr,\n",
    "    cv=5, # 5-кратная кросс-валидация\n",
    "    scoring=scoring_metrics,\n",
    "    refit='R2', # Оптимизировать по R2\n",
    "    n_jobs=-1, # Использовать все доступные ядра\n",
    "    verbose=1 # Выводить прогресс\n",
    ")\n",
    "\n",
    "grid_search_lr.fit(X_train_si, y_train_si)\n",
    "\n",
    "print(\"Лучшие параметры для Линейной Регрессии:\", grid_search_lr.best_params_)\n",
    "print(\"Лучший R2 на кросс-валидации:\", grid_search_lr.best_score_)\n",
    "\n",
    "# Оценка на тестовой выборке\n",
    "y_pred_lr = grid_search_lr.predict(X_test_si)\n",
    "mse_lr = mean_squared_error(y_test_si, y_pred_lr)\n",
    "r2_lr = r2_score(y_test_si, y_pred_lr)\n",
    "mae_lr = mean_absolute_error(y_test_si, y_pred_lr)\n",
    "\n",
    "print(\"\\nМетрики на тестовой выборке (Линейная Регрессия):\")\n",
    "print(f\"MSE: {mse_lr:.4f}\")\n",
    "print(f\"MAE: {mae_lr:.4f}\")\n",
    "print(f\"R2: {r2_lr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca8e93bf-7672-4a9f-888d-53998c1258aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Random Forest Регрессор для SI =====\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Лучшие параметры для Random Forest: {'regressor__max_features': 0.6, 'regressor__min_samples_leaf': 5, 'regressor__n_estimators': 200}\n",
      "Лучший R2 на кросс-валидации: 0.20402322576465304\n",
      "\n",
      "Метрики на тестовой выборке (Random Forest):\n",
      "MSE: 0.4925\n",
      "MAE: 0.5183\n",
      "R2: 0.1348\n"
     ]
    }
   ],
   "source": [
    "# Модель 2: Random Forest Регрессор \n",
    "print(\"\\n===== Random Forest Регрессор для SI =====\")\n",
    "pipeline_rf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Гиперпараметры для настройки Random Forest\n",
    "param_grid_rf = {\n",
    "    'regressor__n_estimators': [50, 100, 200],\n",
    "    'regressor__max_features': [0.6, 0.8, 1.0],\n",
    "    'regressor__min_samples_leaf': [5, 10]\n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(\n",
    "    pipeline_rf,\n",
    "    param_grid_rf,\n",
    "    cv=5,\n",
    "    scoring=scoring_metrics,\n",
    "    refit='R2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_rf.fit(X_train_si, y_train_si)\n",
    "\n",
    "print(\"Лучшие параметры для Random Forest:\", grid_search_rf.best_params_)\n",
    "print(\"Лучший R2 на кросс-валидации:\", grid_search_rf.best_score_)\n",
    "\n",
    "# Оценка на тестовой выборке\n",
    "y_pred_rf = grid_search_rf.predict(X_test_si)\n",
    "mse_rf = mean_squared_error(y_test_si, y_pred_rf)\n",
    "r2_rf = r2_score(y_test_si, y_pred_rf)\n",
    "mae_rf = mean_absolute_error(y_test_si, y_pred_rf)\n",
    "\n",
    "print(\"\\nМетрики на тестовой выборке (Random Forest):\")\n",
    "print(f\"MSE: {mse_rf:.4f}\")\n",
    "print(f\"MAE: {mae_rf:.4f}\")\n",
    "print(f\"R2: {r2_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0abd37f6-bdd4-41b0-b6f4-dee72212bae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== LightGBM Регрессор для SI =====\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000947 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9262\n",
      "[LightGBM] [Info] Number of data points in the train set: 773, number of used features: 62\n",
      "[LightGBM] [Info] Start training from score 0.758859\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Лучшие параметры для LightGBM: {'regressor__learning_rate': 0.05, 'regressor__n_estimators': 100, 'regressor__num_leaves': 31, 'regressor__reg_alpha': 0.5, 'regressor__reg_lambda': 0.5}\n",
      "Лучший R2 на кросс-валидации: 0.17923221832518094\n",
      "\n",
      "Метрики на тестовой выборке (LightGBM):\n",
      "MSE: 0.5191\n",
      "MAE: 0.5227\n",
      "R2: 0.0880\n"
     ]
    }
   ],
   "source": [
    "# Модель 3: LightGBM Регрессор\n",
    "print(\"\\n===== LightGBM Регрессор для SI =====\")\n",
    "pipeline_lgbm = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LGBMRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Гиперпараметры для настройки LightGBM\n",
    "param_grid_lgbm = {\n",
    "    'regressor__n_estimators': [100, 200, 300],\n",
    "    'regressor__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'regressor__num_leaves': [31, 63],\n",
    "    'regressor__reg_alpha': [0.1, 0.5],\n",
    "    'regressor__reg_lambda': [0.1, 0.5]\n",
    "}\n",
    "\n",
    "grid_search_lgbm = GridSearchCV(\n",
    "    pipeline_lgbm,\n",
    "    param_grid_lgbm,\n",
    "    cv=5,\n",
    "    scoring=scoring_metrics,\n",
    "    refit='R2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_lgbm.fit(X_train_si, y_train_si)\n",
    "\n",
    "print(\"Лучшие параметры для LightGBM:\", grid_search_lgbm.best_params_)\n",
    "print(\"Лучший R2 на кросс-валидации:\", grid_search_lgbm.best_score_)\n",
    "\n",
    "# Оценка на тестовой выборке\n",
    "y_pred_lgbm = grid_search_lgbm.predict(X_test_si)\n",
    "mse_lgbm = mean_squared_error(y_test_si, y_pred_lgbm)\n",
    "r2_lgbm = r2_score(y_test_si, y_pred_lgbm)\n",
    "mae_lgbm = mean_absolute_error(y_test_si, y_pred_lgbm)\n",
    "\n",
    "print(\"\\nМетрики на тестовой выборке (LightGBM):\")\n",
    "print(f\"MSE: {mse_lgbm:.4f}\")\n",
    "print(f\"MAE: {mae_lgbm:.4f}\")\n",
    "print(f\"R2: {r2_lgbm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "575f0ce4-0395-4689-a63c-0d3d4ecce9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== XGBoost Регрессор для SI =====\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Лучшие параметры для XGBoost: {'regressor__colsample_bytree': 0.7, 'regressor__learning_rate': 0.01, 'regressor__max_depth': 7, 'regressor__n_estimators': 200, 'regressor__subsample': 0.7}\n",
      "Лучший R2 на кросс-валидации: 0.2142045631738454\n",
      "\n",
      "Метрики на тестовой выборке (XGBoost):\n",
      "MSE: 0.4890\n",
      "MAE: 0.5244\n",
      "R2: 0.1409\n"
     ]
    }
   ],
   "source": [
    "# Модель 4: XGBoost Regressor\n",
    "print(\"\\n===== XGBoost Регрессор для SI =====\")\n",
    "\n",
    "pipeline_xgb = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', XGBRegressor(random_state=42, eval_metric='rmse')) # eval_metric для предотвращения предупреждений\n",
    "])\n",
    "\n",
    "# Гиперпараметры для настройки XGBoost\n",
    "param_grid_xgb = {\n",
    "    'regressor__n_estimators': [100, 200, 300],\n",
    "    'regressor__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'regressor__max_depth': [3, 5, 7],\n",
    "    'regressor__subsample': [0.7, 1.0], # Доля выборок для обучения деревьев\n",
    "    'regressor__colsample_bytree': [0.7, 1.0] # Доля признаков для каждого дерева\n",
    "}\n",
    "\n",
    "grid_search_xgb = GridSearchCV(\n",
    "    pipeline_xgb,\n",
    "    param_grid_xgb,\n",
    "    cv=5,\n",
    "    scoring=scoring_metrics,\n",
    "    refit='R2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_xgb.fit(X_train_si, y_train_si)\n",
    "\n",
    "print(\"Лучшие параметры для XGBoost:\", grid_search_xgb.best_params_)\n",
    "print(\"Лучший R2 на кросс-валидации:\", grid_search_xgb.best_score_)\n",
    "\n",
    "# Оценка на тестовой выборке\n",
    "y_pred_xgb = grid_search_xgb.predict(X_test_si)\n",
    "mse_xgb = mean_squared_error(y_test_si, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test_si, y_pred_xgb)\n",
    "mae_xgb = mean_absolute_error(y_test_si, y_pred_xgb)\n",
    "\n",
    "print(\"\\nМетрики на тестовой выборке (XGBoost):\")\n",
    "print(f\"MSE: {mse_xgb:.4f}\")\n",
    "print(f\"MAE: {mae_xgb:.4f}\")\n",
    "print(f\"R2: {r2_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "721780c5-9bb3-4112-93c2-598ef4ae9227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== CatBoost Регрессор для SI =====\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "Лучшие параметры для CatBoost: {'regressor__depth': 8, 'regressor__iterations': 100, 'regressor__l2_leaf_reg': 1, 'regressor__learning_rate': 0.05}\n",
      "Лучший R2 на кросс-валидации: 0.21601061696531523\n",
      "\n",
      "Метрики на тестовой выборке (CatBoost):\n",
      "MSE: 0.4856\n",
      "MAE: 0.5176\n",
      "R2: 0.1468\n"
     ]
    }
   ],
   "source": [
    "# Модель 5: CatBoost Regressor\n",
    "print(\"\\n===== CatBoost Регрессор для SI =====\")\n",
    "\n",
    "pipeline_cat = Pipeline([\n",
    "    ('scaler', StandardScaler()), # CatBoost менее чувствителен к масштабированию, но для пайплайна оставляем\n",
    "    ('regressor', CatBoostRegressor(random_state=42, verbose=0)) # verbose=0 отключает вывод логов CatBoost\n",
    "])\n",
    "\n",
    "# Гиперпараметры для настройки CatBoost\n",
    "param_grid_cat = {\n",
    "    'regressor__iterations': [100, 200, 300], # Количество итераций (деревьев)\n",
    "    'regressor__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'regressor__depth': [4, 6, 8], # Глубина деревьев\n",
    "    'regressor__l2_leaf_reg': [1, 3, 5] # L2 регуляризация\n",
    "}\n",
    "\n",
    "grid_search_cat = GridSearchCV(\n",
    "    pipeline_cat,\n",
    "    param_grid_cat,\n",
    "    cv=5,\n",
    "    scoring=scoring_metrics,\n",
    "    refit='R2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_cat.fit(X_train_si, y_train_si)\n",
    "\n",
    "print(\"Лучшие параметры для CatBoost:\", grid_search_cat.best_params_)\n",
    "print(\"Лучший R2 на кросс-валидации:\", grid_search_cat.best_score_)\n",
    "\n",
    "# Оценка на тестовой выборке\n",
    "y_pred_cat = grid_search_cat.predict(X_test_si)\n",
    "mse_cat = mean_squared_error(y_test_si, y_pred_cat)\n",
    "r2_cat = r2_score(y_test_si, y_pred_cat)\n",
    "mae_cat = mean_absolute_error(y_test_si, y_pred_cat)\n",
    "\n",
    "print(\"\\nМетрики на тестовой выборке (CatBoost):\")\n",
    "print(f\"MSE: {mse_cat:.4f}\")\n",
    "print(f\"MAE: {mae_cat:.4f}\")\n",
    "print(f\"R2: {r2_cat:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "554a4f51-60d0-4709-b422-bee1a90d1401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Простая Нейронная Сеть для SI =====\n",
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры для Нейронной Сети: {'regressor__activation': 'relu', 'regressor__batch_size': 64, 'regressor__epochs': 50, 'regressor__hidden_layers': 2, 'regressor__learning_rate': 0.001, 'regressor__neurons': 64, 'regressor__optimizer': 'adam'}\n",
      "Лучший R2 на кросс-валидации: 0.19051994487952806\n",
      "\n",
      "Метрики на тестовой выборке (Нейронная Сеть):\n",
      "MSE: 0.6024\n",
      "MAE: 0.5545\n",
      "R2: -0.0584\n"
     ]
    }
   ],
   "source": [
    "# Модель 6: Простая Нейронная Сеть (Keras Sequential)\n",
    "\n",
    "print(\"\\n===== Простая Нейронная Сеть для SI =====\")\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers as L\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "# Функция для создания Keras-модели\n",
    "# Параметры, которые мы хотим усовершенствовать через GridSearchCV, должны быть аргументами этой функции.\n",
    "def build_nn_model(meta, hidden_layers=1, neurons=32, activation='relu',\n",
    "                     optimizer='adam', learning_rate=0.001):\n",
    "    n_features = meta[\"n_features_in_\"]\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(L.Input(shape=(n_features,)))\n",
    "    \n",
    "    for _ in range(hidden_layers):\n",
    "        model.add(L.Dense(neurons, activation=activation))\n",
    "        \n",
    "    model.add(L.Dense(1)) # Выходной слой для регрессии (1 нейрон, без активации)\n",
    "    \n",
    "    # Создаем экземпляр оптимизатора с указанной скоростью обучения\n",
    "    if optimizer == 'adam':\n",
    "        opt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        opt = keras.optimizers.Adam(learning_rate=learning_rate) # По умолчанию Adam\n",
    "\n",
    "    model.compile(optimizer=opt, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Здесь мы указываем KerasRegressor\n",
    "pipeline_nn = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', KerasRegressor(\n",
    "        model=build_nn_model,\n",
    "        # Задаем дефолтные значения для KerasRegressor, они будут переопределяться GridSearchCV\n",
    "        # Важно: эти параметры должны быть аргументами build_nn_model\n",
    "        hidden_layers=1,\n",
    "        neurons=32,\n",
    "        activation='relu',\n",
    "        optimizer='adam',\n",
    "        learning_rate=0.001,\n",
    "        batch_size=32, # batch_size и epochs - это параметры .fit(), а не .build_model()\n",
    "        epochs=50,\n",
    "        verbose=0,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Гиперпараметры для настройки Нейронной Сети\n",
    "# Gараметры указываются с префиксом 'regressor__' для шага 'regressor' в пайплайне\n",
    "# Эти параметры будут переданы в конструктор KerasRegressor, который затем передаст их в build_nn_model.\n",
    "param_grid_nn = {\n",
    "    'regressor__hidden_layers': [1, 2],\n",
    "    'regressor__neurons': [32, 64],\n",
    "    'regressor__activation': ['relu'],\n",
    "    'regressor__optimizer': ['adam'],\n",
    "    'regressor__learning_rate': [0.001, 0.01],\n",
    "    'regressor__batch_size': [32, 64],\n",
    "    'regressor__epochs': [50, 100]\n",
    "}\n",
    "\n",
    "grid_search_nn = GridSearchCV(\n",
    "    pipeline_nn,\n",
    "    param_grid_nn,\n",
    "    cv=3,\n",
    "    scoring=scoring_metrics,\n",
    "    refit='R2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_nn.fit(X_train_si, y_train_si)\n",
    "\n",
    "print(\"Лучшие параметры для Нейронной Сети:\", grid_search_nn.best_params_)\n",
    "print(\"Лучший R2 на кросс-валидации:\", grid_search_nn.best_score_)\n",
    "\n",
    "# Оценка на тестовой выборке\n",
    "y_pred_nn = grid_search_nn.predict(X_test_si)\n",
    "mse_nn = mean_squared_error(y_test_si, y_pred_nn)\n",
    "r2_nn = r2_score(y_test_si, y_pred_nn)\n",
    "mae_nn = mean_absolute_error(y_test_si, y_pred_nn)\n",
    "\n",
    "print(\"\\nМетрики на тестовой выборке (Нейронная Сеть):\")\n",
    "print(f\"MSE: {mse_nn:.4f}\")\n",
    "print(f\"MAE: {mae_nn:.4f}\")\n",
    "print(f\"R2: {r2_nn:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623f6708-e558-41a8-a56c-63646484b521",
   "metadata": {},
   "source": [
    "### Анализ результатов для log_SI\n",
    "\n",
    "### Линейная Регрессия:\n",
    "\n",
    "R2 на кросс-валидации: 0.068\n",
    "R2 на тесте: 0.0015\n",
    "Комментарий: R2 практически равен нулю на тестовой выборке, что означает, что линейная модель практически не объясняет дисперсию log_SI.\n",
    "\n",
    "### Random Forest Регрессор:\n",
    "\n",
    "Лучшие параметры: max_features=0.6, min_samples_leaf=5, n_estimators=200\n",
    "R2 на кросс-валидации: 0.204\n",
    "R2 на тесте: 0.1348\n",
    "Комментарий: Лучше линейной регрессии, но R2 все еще очень низкий. Объясняется лишь около 13% дисперсии.\n",
    "\n",
    "### LightGBM Регрессор:\n",
    "\n",
    "Лучшие параметры: learning_rate=0.05, n_estimators=100, num_leaves=31, reg_alpha=0.5, reg_lambda=0.5\n",
    "R2 на кросс-валидации: 0.179\n",
    "R2 на тесте: 0.0880\n",
    "Комментарий: Хуже, чем Random Forest, и R2 на тесте составляет менее 9%, что крайне мало.\n",
    "\n",
    "### XGBoost Регрессор:\n",
    "\n",
    "Лучшие параметры: colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.7\n",
    "R2 на кросс-валидации: 0.214\n",
    "R2 на тесте: 0.1409\n",
    "Комментарий: Немного лучше Random Forest, но R2 все равно очень низкий.\n",
    "\n",
    "### CatBoost Регрессор:\n",
    "\n",
    "Лучшие параметры: depth=8, iterations=100, l2_leaf_reg=1, learning_rate=0.05\n",
    "R2 на кросс-валидации: 0.216\n",
    "R2 на тесте: 0.1468\n",
    "Комментарий: CatBoost также показывает лучшие результаты для log_SI среди всех моделей, но общий уровень R2 остается крайне низким. MAE около 0.5176 все еще высок относительно диапазона значений log_SI.\n",
    "\n",
    "### Простая Нейронная Сеть:\n",
    "\n",
    "Лучшие параметры: activation='relu', batch_size=64, epochs=50, hidden_layers=2, learning_rate=0.001, neurons=64, optimizer='adam'\n",
    "R2 на кросс-валидации: 0.191\n",
    "R2 на тесте: -0.0584\n",
    "Комментарий: Отрицательный R2 на тестовой выборке означает, что модель работает хуже, чем простое предсказание среднего значения log_SI. Это очень плохой результат.\n",
    "\n",
    "## Вывод по log_SI: \n",
    "Результаты регрессии для log_SI действительно очень слабые для всех моделей. CatBoost Regressor показал себя наилучшим образом, но даже его R2 едва превышает 0.14. Это говорит о том, что текущий набор признаков очень плохо объясняет дисперсию log_SI. Как ты и предполагал, что-то влияет очень сильно, но текущие признаки не могут это уловить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76606ac7-75d2-423d-9db5-aee9cd52440e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
